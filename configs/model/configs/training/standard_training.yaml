# configs/training/standard_training.yaml
training:
  # Optimizer
  optimizer: "AdamW"
  learning_rate: 1e-4
  weight_decay: 0.01
  betas: [0.9, 0.98]
  eps: 1e-8
  
  # Learning rate schedule
  lr_scheduler: "cosine_with_warmup"
  warmup_steps: 10000
  max_steps: 500000
  
  # Batch size and gradient
  batch_size: 32
  gradient_accumulation_steps: 4
  gradient_clipping: 1.0
  
  # Regularization
  dropout: 0.1
  attention_dropout: 0.1
  hidden_dropout: 0.1
  
  # Loss function
  loss_function: "cross_entropy"
  label_smoothing: 0.1
  
  # Multi-component loss weights
  task_loss_weight: 1.0
  kl_loss_weight: 0.001
  gate_loss_weight: 0.1
  load_balance_weight: 0.01
  
  # Training stability
  mixed_precision: true
  seed: 42
  deterministic: true
